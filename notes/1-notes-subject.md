# Phase 2. Formulating a research question

Subject Idea 1: Ethical Implications and Frameworks for Autonomous AI Agents
Main Research Question:
What are the ethical implications of deploying autonomous AI agents in decision-making roles, and how can transparency and accountability be ensured in their outputs?

Sub-questions:
Problem Domain:

What specific ethical risks (e.g., bias, discrimination, erosion of human autonomy, lack of due process) are most prevalent and impactful when autonomous AI agents are deployed in decision-making roles within healthcare, finance, and criminal justice?

How do current regulatory frameworks (e.g., EU AI Act, GDPR) and industry standards address the transparency and accountability of autonomous AI agents, and what are the key identified gaps or challenges in their practical application?

What are the primary technical (e.g., model complexity, data opacity) and organizational (e.g., lack of expertise, unclear governance structures) barriers to achieving sufficient transparency and accountability in current AI agent deployments?

Solution Domain:

Which existing Explainable AI (XAI) techniques (e.g., SHAP, LIME, counterfactual explanations) are most effective in enhancing the transparency of decision-making processes for different types of AI agents and in various contexts within the selected sectors?

What novel or adapted governance structures, operational protocols (e.g., real-time decision logging, mandatory ethical impact assessments, dynamic liability models), and technical mechanisms can be integrated into a comprehensive framework to improve accountability for decisions made by autonomous AI agents?

How can human oversight be effectively and meaningfully integrated with autonomous AI decision-making systems to ensure ethical outcomes and maintain human agency, without unduly sacrificing the benefits of automation?

What practical, actionable guidelines and best practices can be developed to assist organizations in designing, implementing, and managing transparent and accountable AI agents in regulated and ethically sensitive environments?

Abstract:
This study investigates the ethical implications of deploying autonomous AI agents in decision-making roles across healthcare, finance, and criminal justice sectors, focusing on transparency and accountability challenges. Through comparative case analysis of three operational AI systems, we aim to identify critical risks including algorithmic bias amplification (e.g., potentially observing disparities similar to a 23% racial disparity in predictive policing outcomes) and accountability gaps in automated loan approval processes. The research will implement a mixed-methods approach combining technical audits of neural network architectures with stakeholder interviews (target Nâ‰ˆ40-50 experts). It is anticipated that explainable AI (XAI) techniques like SHAP values could reduce decision opacity significantly (e.g., by an estimated 40-45%) while maintaining high diagnostic accuracy (e.g., around 90-92%) in simulated clinical trials. A novel accountability framework will be proposed, integrating mandatory real-time decision logging and tiered liability models, which is hypothesized to improve auditability substantially (e.g., by an estimated 60-70%) in financial fraud detection systems. Practical implementations will explore how adversarial debiasing of training data might reduce demographic parity gaps (e.g., by an estimated 30-40%) without compromising model performance. The study aims to conclude with evidence-based policy recommendations, including human oversight requirements for high-stakes decisions and compliance protocols aligned with emerging regulations like the EU AI Act. These findings are intended to provide actionable guidelines for balancing AI autonomy with ethical responsibility, offering a blueprint for organizations implementing autonomous agents in regulated environments.
